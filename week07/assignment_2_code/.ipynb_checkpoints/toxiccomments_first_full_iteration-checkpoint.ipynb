{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toxic comments\n",
    "\n",
    "This notebook takes you though a complete iteration of Machine Learning Assignment 1 - Toxic comments. The assignment details (including links to download the data) can be found [here](https://docs.google.com/document/d/1WGYw99e5q6j5V0Zrf2HveagU6URt_kVvdR8B9HYQ99E/edit?usp=sharing). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all imports and magic commands\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from my_measures import BinaryClassificationPerformance\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORTANT!!! Make sure you are using `BinaryClassificationPerformance` v1.02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class BinaryClassificationPerformance in module my_measures:\n",
      "\n",
      "class BinaryClassificationPerformance(builtins.object)\n",
      " |  BinaryClassificationPerformance(predictions, labels, desc, probabilities=None)\n",
      " |  \n",
      " |  Performance measures to evaluate the fit of a binary classification model, v1.02\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, predictions, labels, desc, probabilities=None)\n",
      " |      Initialize attributes: predictions-vector of predicted values for Y, labels-vector of labels for Y\n",
      " |  \n",
      " |  compute_measures(self)\n",
      " |      Compute performance measures defined by Flach p. 57\n",
      " |  \n",
      " |  img_indices(self)\n",
      " |      Get the indices of true and false positives to be able to locate the corresponding images in a list of image names\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(BinaryClassificationPerformance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function for feature building and extraction on natural language data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that takes raw data and completes all preprocessing required before model fits\n",
    "def process_raw_data(fn, my_random_seed, test=False):\n",
    "    # read and summarize data\n",
    "    toxic_data = pd.read_csv(fn)\n",
    "    if (not test):\n",
    "        # add an indicator for any toxic, severe toxic, obscene, threat, insult, or indentity hate\n",
    "        toxic_data['any_toxic'] = (toxic_data['toxic'] + toxic_data['severe_toxic'] + toxic_data['obscene'] + toxic_data['threat'] + toxic_data['insult'] + toxic_data['identity_hate'] > 0)\n",
    "    print(\"toxic_data is:\", type(toxic_data))\n",
    "    print(\"toxic_data has\", toxic_data.shape[0], \"rows and\", toxic_data.shape[1], \"columns\", \"\\n\")\n",
    "    print(\"the data types for each of the columns in toxic_data:\")\n",
    "    print(toxic_data.dtypes, \"\\n\")\n",
    "    print(\"the first 10 rows in toxic_data:\")\n",
    "    print(toxic_data.head(5))\n",
    "    if (not test):\n",
    "        print(\"The rate of 'toxic' Wikipedia comments in the dataset: \")\n",
    "        print(toxic_data['any_toxic'].mean())\n",
    "\n",
    "    # vectorize Bag of Words from review text; as sparse matrix\n",
    "    if (not test): # fit_transform()\n",
    "        hv = HashingVectorizer(n_features=2 ** 22, alternate_sign=False)\n",
    "        X_hv = hv.fit_transform(toxic_data.comment_text)\n",
    "        fitted_transformations.append(hv)\n",
    "        print(\"Shape of HashingVectorizer X:\")\n",
    "        print(X_hv.shape)\n",
    "    else: # transform() \n",
    "        X_hv = fitted_transformations[0].transform(toxic_data.comment_text)\n",
    "        print(\"Shape of HashingVectorizer X:\")\n",
    "        print(X_hv.shape)\n",
    "    \n",
    "    # http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html\n",
    "    if (not test):\n",
    "        transformer = TfidfTransformer()\n",
    "        X_tfidf = transformer.fit_transform(X_hv)\n",
    "        fitted_transformations.append(transformer)\n",
    "    else:\n",
    "        X_tfidf = fitted_transformations[1].transform(X_hv)\n",
    "    \n",
    "    # create additional quantitative features\n",
    "    # features from Amazon.csv to add to feature set\n",
    "    toxic_data['word_count'] = toxic_data['comment_text'].str.split(' ').str.len()\n",
    "    toxic_data['punc_count'] = toxic_data['comment_text'].str.count(\"\\.\")\n",
    "\n",
    "    X_quant_features = toxic_data[[\"word_count\", \"punc_count\"]]\n",
    "    print(\"Look at a few rows of the new quantitative features: \")\n",
    "    print(X_quant_features.head(10))\n",
    "    \n",
    "    # Combine all quantitative features into a single sparse matrix\n",
    "    X_quant_features_csr = csr_matrix(X_quant_features)\n",
    "    X_combined = hstack([X_tfidf, X_quant_features_csr])\n",
    "    X_matrix = csr_matrix(X_combined) # convert to sparse matrix\n",
    "    print(\"Size of combined bag of words and new quantitative variables matrix:\")\n",
    "    print(X_matrix.shape)\n",
    "    \n",
    "    # Create `X`, scaled matrix of features\n",
    "    # feature scaling\n",
    "    if (not test):\n",
    "        sc = StandardScaler(with_mean=False)\n",
    "        X = sc.fit_transform(X_matrix)\n",
    "        fitted_transformations.append(sc)\n",
    "        print(X.shape)\n",
    "        y = toxic_data['any_toxic']\n",
    "    else:\n",
    "        X = fitted_transformations[2].transform(X_matrix)\n",
    "        print(X.shape)\n",
    "    \n",
    "    # Create Training and Test Sets\n",
    "    # enter an integer for the random_state parameter; any integer will work\n",
    "    if (test):\n",
    "        X_submission_test = X\n",
    "        print(\"Shape of X_test for submission:\")\n",
    "        print(X_submission_test.shape)\n",
    "        print('SUCCESS!')\n",
    "        return(toxic_data, X_submission_test)\n",
    "    else: \n",
    "        X_train, X_test, y_train, y_test, X_raw_train, X_raw_test = train_test_split(X, y, toxic_data, test_size=0.2, random_state=my_random_seed)\n",
    "        print(\"Shape of X_train and X_test:\")\n",
    "        print(X_train.shape)\n",
    "        print(X_test.shape)\n",
    "        print(\"Shape of y_train and y_test:\")\n",
    "        print(y_train.shape)\n",
    "        print(y_test.shape)\n",
    "        print(\"Shape of X_raw_train and X_raw_test:\")\n",
    "        print(X_raw_train.shape)\n",
    "        print(X_raw_test.shape)\n",
    "        print('SUCCESS!')\n",
    "        return(X_train, X_test, y_train, y_test, X_raw_train, X_raw_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create training and test sets from function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxic_data is: <class 'pandas.core.frame.DataFrame'>\n",
      "toxic_data has 159571 rows and 9 columns \n",
      "\n",
      "the data types for each of the columns in toxic_data:\n",
      "id               object\n",
      "comment_text     object\n",
      "toxic             int64\n",
      "severe_toxic      int64\n",
      "obscene           int64\n",
      "threat            int64\n",
      "insult            int64\n",
      "identity_hate     int64\n",
      "any_toxic          bool\n",
      "dtype: object \n",
      "\n",
      "the first 10 rows in toxic_data:\n",
      "                 id                                       comment_text  toxic  \\\n",
      "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
      "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
      "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
      "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
      "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
      "\n",
      "   severe_toxic  obscene  threat  insult  identity_hate  any_toxic  \n",
      "0             0        0       0       0              0      False  \n",
      "1             0        0       0       0              0      False  \n",
      "2             0        0       0       0              0      False  \n",
      "3             0        0       0       0              0      False  \n",
      "4             0        0       0       0              0      False  \n",
      "The rate of 'toxic' Wikipedia comments in the dataset: \n",
      "0.10167887648758234\n",
      "Shape of HashingVectorizer X:\n",
      "(159571, 4194304)\n",
      "Look at a few rows of the new quantitative features: \n",
      "   word_count  punc_count\n",
      "0          42           5\n",
      "1          18           2\n",
      "2          42           3\n",
      "3         112           3\n",
      "4          13           1\n",
      "5          12           1\n",
      "6           8           0\n",
      "7          21           2\n",
      "8          83           7\n",
      "9          12           0\n",
      "Size of combined bag of words and new quantitative variables matrix:\n",
      "(159571, 4194306)\n",
      "(159571, 4194306)\n",
      "Shape of X_train and X_test:\n",
      "(127656, 4194306)\n",
      "(31915, 4194306)\n",
      "Shape of y_train and y_test:\n",
      "(127656,)\n",
      "(31915,)\n",
      "Shape of X_raw_train and X_raw_test:\n",
      "(127656, 11)\n",
      "(31915, 11)\n",
      "SUCCESS!\n",
      "Number of fits stored in `fitted_transformations` list: \n",
      "3\n"
     ]
    }
   ],
   "source": [
    "# create an empty list to store any use of fit_transform() to transform() later\n",
    "# it is a global list to store model and feature extraction fits\n",
    "fitted_transformations = []\n",
    "\n",
    "# CHANGE FILE PATH and my_random_seed number (any integer other than 74 will do): \n",
    "X_train, X_test, y_train, y_test, X_raw_train, X_raw_test = process_raw_data(fn = r\"C:\\Users\\miame\\Documents\\000UNI\\000PARSONS\\MACHINE LEARNING\\CODES\\ml\\final_assignment_1\\toxiccomments_train.csv\", my_random_seed=42)\n",
    "\n",
    "print(\"Number of fits stored in `fitted_transformations` list: \")\n",
    "print(len(fitted_transformations))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit (and tune) Various Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL: ordinary least squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "ols = linear_model.SGDClassifier(loss=\"squared_loss\")\n",
    "ols.fit(X_train, y_train)\n",
    "\n",
    "ols_performance_train = BinaryClassificationPerformance(ols.predict(X_train), y_train, 'ols_train')\n",
    "ols_performance_train.compute_measures()\n",
    "print(ols_performance_train.performance_measures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL: SVM, linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "svm = linear_model.SGDClassifier()\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "svm_performance_train = BinaryClassificationPerformance(svm.predict(X_train), y_train, 'svm_train')\n",
    "svm_performance_train.compute_measures()\n",
    "print(svm_performance_train.performance_measures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL: logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "lgs = linear_model.SGDClassifier(loss='log')\n",
    "lgs.fit(X_train, y_train)\n",
    "\n",
    "lgs_performance_train = BinaryClassificationPerformance(lgs.predict(X_train), y_train, 'lgs_train')\n",
    "lgs_performance_train.compute_measures()\n",
    "print(lgs_performance_train.performance_measures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL: Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nbs = MultinomialNB()\n",
    "nbs.fit(X_train, y_train)\n",
    "\n",
    "nbs_performance_train = BinaryClassificationPerformance(nbs.predict(X_train), y_train, 'nbs_train')\n",
    "nbs_performance_train.compute_measures()\n",
    "print(nbs_performance_train.performance_measures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL: Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "prc = linear_model.SGDClassifier(loss='perceptron')\n",
    "prc.fit(X_train, y_train)\n",
    "\n",
    "prc_performance_train = BinaryClassificationPerformance(prc.predict(X_train), y_train, 'prc_train')\n",
    "prc_performance_train.compute_measures()\n",
    "print(prc_performance_train.performance_measures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL: Ridge Regression Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Pos': 12981, 'Neg': 114675, 'TP': 6838, 'TN': 114669, 'FP': 6, 'FN': 6143, 'Accuracy': 0.9518314846149025, 'Precision': 0.9991233196960841, 'Recall': 0.5267698944611355, 'desc': 'rdg_train'}\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "rdg = linear_model.RidgeClassifier(300000) # provide alpha value here; default is 1.0\n",
    "rdg.fit(X_train, y_train)\n",
    "\n",
    "rdg_performance_train = BinaryClassificationPerformance(rdg.predict(X_train), y_train, 'rdg_train')\n",
    "rdg_performance_train.compute_measures()\n",
    "print(rdg_performance_train.performance_measures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL: Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rdf = RandomForestClassifier(max_depth=2, random_state=0)\n",
    "rdf.fit(X_train, y_train)\n",
    "\n",
    "rdf_performance_train = BinaryClassificationPerformance(rdf.predict(X_train), y_train, 'rdf_train')\n",
    "rdf_performance_train.compute_measures()\n",
    "print(rdf_performance_train.performance_measures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC plot to compare performance of various models and fits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fits = [ols_performance_train, svm_performance_train, lgs_performance_train, nbs_performance_train, prc_performance_train, rdg_performance_train, rdf_performance_train]\n",
    "\n",
    "for fit in fits:\n",
    "    plt.plot(fit.performance_measures['FP'] / fit.performance_measures['Neg'], \n",
    "             fit.performance_measures['TP'] / fit.performance_measures['Pos'], 'bo')\n",
    "    plt.text(fit.performance_measures['FP'] / fit.performance_measures['Neg'], \n",
    "             fit.performance_measures['TP'] / fit.performance_measures['Pos'], fit.desc)\n",
    "plt.axis([0, 1, 0, 1])\n",
    "plt.title('ROC plot: test set')\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### looking at reviews based on their classification\n",
    "\n",
    "Let's say we decide that Ordinary Least Squares (OLS) Regression is the best model for generalization. Let's take a look at some of the reviews and try to make a (subjective) determination of whether it's generalizing well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### let's look at some false positives:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ols_predictions = ols.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# false positives\n",
    "\n",
    "print(\"Examples of false positives:\")\n",
    "\n",
    "import random, time\n",
    "\n",
    "for i in range(0, len(ols_predictions)):\n",
    "    if (ols_predictions[i] == 1):\n",
    "        if (X_raw_train.iloc[i]['any_toxic'] == 0):\n",
    "            if (random.uniform(0, 1) < 0.05): # to print only 5% of the false positives\n",
    "                print(i)\n",
    "                print(X_raw_train.iloc[i]['comment_text'])\n",
    "                print('* * * * * * * * * ')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# <span style=\"color:red\">WARNING: Don't look at test set performance too much!</span>\n",
    "\n",
    "---\n",
    "\n",
    "The following cells show performance on your test set. Do not look at this too often! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Look at performance on the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL: ordinary least squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ols_performance_test = BinaryClassificationPerformance(ols.predict(X_test), y_test, 'ols_test')\n",
    "ols_performance_test.compute_measures()\n",
    "print(ols_performance_test.performance_measures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL: SVM, linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_performance_test = BinaryClassificationPerformance(svm.predict(X_test), y_test, 'svm_test')\n",
    "svm_performance_test.compute_measures()\n",
    "print(svm_performance_test.performance_measures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL: logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgs_performance_test = BinaryClassificationPerformance(lgs.predict(X_test), y_test, 'lgs_test')\n",
    "lgs_performance_test.compute_measures()\n",
    "print(lgs_performance_test.performance_measures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL: Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbs_performance_test = BinaryClassificationPerformance(nbs.predict(X_test), y_test, 'nbs_test')\n",
    "nbs_performance_test.compute_measures()\n",
    "print(nbs_performance_test.performance_measures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL: Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prc_performance_test = BinaryClassificationPerformance(prc.predict(X_test), y_test, 'prc_test')\n",
    "prc_performance_test.compute_measures()\n",
    "print(prc_performance_test.performance_measures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL: Ridge Regression Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Pos': 3244, 'Neg': 28671, 'TP': 705, 'TN': 28658, 'FP': 13, 'FN': 2539, 'Accuracy': 0.920037599874667, 'Precision': 0.9818941504178273, 'Recall': 0.21732429099876696, 'desc': 'rdg_test'}\n"
     ]
    }
   ],
   "source": [
    "rdg_performance_test = BinaryClassificationPerformance(rdg.predict(X_test), y_test, 'rdg_test')\n",
    "rdg_performance_test.compute_measures()\n",
    "print(rdg_performance_test.performance_measures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL: Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdf_performance_test = BinaryClassificationPerformance(rdf.predict(X_test), y_test, 'rdf_test')\n",
    "rdf_performance_test.compute_measures()\n",
    "print(rdf_performance_test.performance_measures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC plot to compare performance of various models and fits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ols_performance_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-c660a16916b2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mols_performance_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msvm_performance_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlgs_performance_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnbs_performance_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprc_performance_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrdg_performance_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrdf_performance_test\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfit\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfits\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     plt.plot(fit.performance_measures['FP'] / fit.performance_measures['Neg'], \n\u001b[0;32m      4\u001b[0m              fit.performance_measures['TP'] / fit.performance_measures['Pos'], 'bo')\n\u001b[0;32m      5\u001b[0m     plt.text(fit.performance_measures['FP'] / fit.performance_measures['Neg'], \n",
      "\u001b[1;31mNameError\u001b[0m: name 'ols_performance_test' is not defined"
     ]
    }
   ],
   "source": [
    "fits = [ols_performance_test, svm_performance_test, lgs_performance_test, nbs_performance_test, prc_performance_test, rdg_performance_test, rdf_performance_test]\n",
    "for fit in fits:\n",
    "    plt.plot(fit.performance_measures['FP'] / fit.performance_measures['Neg'], \n",
    "             fit.performance_measures['TP'] / fit.performance_measures['Pos'], 'bo')\n",
    "    plt.text(fit.performance_measures['FP'] / fit.performance_measures['Neg'], \n",
    "             fit.performance_measures['TP'] / fit.performance_measures['Pos'], fit.desc)\n",
    "plt.axis([0, 1, 0, 1])\n",
    "plt.title('ROC plot: test set')\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ROC for both train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fits = [rdg_performance_train]\n",
    "\n",
    "for fit in fits:\n",
    "    plt.plot(fit.performance_measures['FP'] / fit.performance_measures['Neg'], \n",
    "             fit.performance_measures['TP'] / fit.performance_measures['Pos'], 'bo')\n",
    "    plt.text(fit.performance_measures['FP'] / fit.performance_measures['Neg'], \n",
    "             fit.performance_measures['TP'] / fit.performance_measures['Pos'], fit.desc)\n",
    "#plt.axis([0, 1, 0, 1])\n",
    "#plt.title('ROC plot: test set')\n",
    "#plt.xlabel('False positive rate')\n",
    "#plt.ylabel('True positive rate')\n",
    "#plt.show()\n",
    "\n",
    "\n",
    "fits = [rdg_performance_test]\n",
    "for fit in fits:\n",
    "    plt.plot(fit.performance_measures['FP'] / fit.performance_measures['Neg'], \n",
    "             fit.performance_measures['TP'] / fit.performance_measures['Pos'], 'rx')\n",
    "    plt.text(fit.performance_measures['FP'] / fit.performance_measures['Neg'], \n",
    "             fit.performance_measures['TP'] / fit.performance_measures['Pos'], fit.desc)\n",
    "plt.axis([0, 1, 0, 1])\n",
    "plt.title('ROC plot: test set')\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# <span style=\"color:red\">SUBMISSION</span>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# read in test data for submission\n",
    "# CHANGE FILE PATH and my_random_seed number (any integer other than 74 will do): \n",
    "raw_data, X_test_submission = process_raw_data(fn = r\"C:\\Users\\miame\\Documents\\000UNI\\000PARSONS\\MACHINE LEARNING\\CODES\\ml\\final_assignment_1\\toxiccomments_test.csv\", my_random_seed=42, test=True)\n",
    "print(\"Number of rows in the submission test set (should be 153,164): \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Choose a <span style=\"color:red\">*single*</span> model for your submission. In this code, I am choosing the Ordinary Least Squares model fit, which is in the `ols` object. But you should choose the model that is performing the best for you! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the id from the raw data\n",
    "my_submission = pd.DataFrame(raw_data[\"id\"])\n",
    "# concatenate predictions to the id\n",
    "my_submission[\"prediction\"] = nbs.predict(X_test_submission)\n",
    "# look at the proportion of positive predictions\n",
    "print(my_submission['prediction'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_submission.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export submission file as pdf\n",
    "# CHANGE FILE PATH: \n",
    "my_submission.to_csv(\"C:\\\\Users\\\\miame\\\\Documents\\\\000UNI\\\\000PARSONS\\\\MACHINE LEARNING\\\\CODES\\\\ml\\\\final_assignment_1\\\\toxiccomments_submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submit to Canvas: 1) the CSV file that was written in the previous cell and 2) the url to the repository (GitHub or other) that contains your code and documentation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
